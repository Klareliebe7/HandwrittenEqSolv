I.INTRODUCTION AND MOTIVATION

kkkk





II. state of the art / related work


















III.your approach / design, implementation and evaluations

To get a better overview of the whole process we have implemented both models that we found in the papers by reproduced the codes in GitHub. After the training process, both models can't reach the theoretical performance that is posted in the paper. For this result, we have speculated the reason for this unsatisfactory. First, we are training both models on a rather small dataset(we have only 8835 items in the dataset), it's difficult to get a well-performed model when we are lack of data to feed the model. Second, neither model is published by the writer of the papers. So there may be some small changes in the model, and there may be plenty of improvements that we can make. Since the Multi-Scale Attention with Dense Encoder model has a better theoretical performance than the WAP model, so the work we're working on is basically based on Multi-Scale Attention with Dense Encoder model. 
The original model form GitHub has an encoder-decoder structure. The encoder part contains 2 different inputs, one has the normal resolution so the Densnet and CNN can extract the information of the structure and the symbols with bigger size, and the other has much higher resolution and it can extract the information about some details that may be neglected by in normal resolution. And the attention mechanism is applied in two different scales, by using attention, the model can focus on the informative annotations of the input images. In the decoder part, the model uses GRUs since the output is the latex form sequences, so the spatial relation plays a very important role here, GRUs can deal with such a problem.  After the GRUs follow the beam search, and it has a beamwidth of 5. And the result is the one with the highest probability. The model has the input image size of 128*128 pixels, and the output is a latex form equation, the tokens in the sequence has 118 different types.
After understanding the model we started to try to make the performance better. After a few tries, we got several models by applying different learning rate changing functions, the best model we got has the WER(Word Error Rate) of 40.93% and the Correct expressions rate of 1.13%, and it's still far from the theoretical performance(WER 12.9%, correct expressions rate 52.8%). So we want to analyze the cause of this poor performance. 
So intuitively we come up with 2 solutions. Firstly is more training data, secondly is higher resolution. To justify those two solutions, we make some statistical work to see which type of sequence or token is more likely to be mispredicted. Besides, we also checked the fault predicted sequences to see if there is any feature that is in common. 
The statistical work is divided into two parts, one is the analysis on the sequence level, and the other one is on the token level. 
Sequence analysis:
We counted the length of the actual sequences, the length of the predicted sequences, and the Levenshtein distance between those corresponding sequences. And we counted these three distances in two different conditions: 1. full sequence that includes all the spatial tokens, it can provide an overview of the error source 2. The symbol only, it can provide the error source of the symbol itself.
The pics below show the distribution of sequences with different lengths.
 

Pic1:
Pic2:
From the comparison of two distributions, we can find out that the number of long sequences in the actual label is more than the number of long sequences in the predicted results. That means that the long sequences may be recognized as some shorter sequences. And the sequence length distribution that is between 0 to 10 has some significant change. That could mean the images that contain very little information may be also recognized wrongly. Which means that the data preprocessing is possibly needed, so that the image that contains the few tokens can be resized into normal size.


Pic3:

From the distribution of the Levenshtein distance, we can draw the conclusion: most of the error length stays in the intervals of 0 to 10, but there still are many severe mistakes that have the Levenshtein distance that’s bigger than 20, which means a total false.
 
 
Tokens analysis:

 
 
 
 
Analysis of the error predictions that have severe problems:
We checked the pics that have big Levenshtein distance, if the distance is smaller than 5, we just consider it as some acceptable error. After sampling from the error predictions, we found some common features of the mispredicted sequences: either the formula has some complicated spatial structure, or the formula is very long, that with the resolution of 128*128 it’s even hard for a human to distinguish the symbols in the image. And some other kind of errors often occurs, one important type is similar tokens, like ‘z’ and ‘2’, uppercase tokens and lowercase tokens, etc.
 
With the hypotheses above we tried the model with higher resolution(256*256) and more data, the outcome is better than the previous model. This model has the WER of 32.29% and the correct expression rate of 16.04%.
 









IV.discussion and conclusion







