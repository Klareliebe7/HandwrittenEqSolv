I.INTRODUCTION AND MOTIVATION

kkkk





II. state of the art / related work



















(iii) your approach / design, implementation and evaluation, 
To get a better overview of the whole process, we have ran both models that we found in the papers that are implemented by other authers in GitHub(https://github.com/jungomi/math-formula-recognition.git, https://github.com/whywhs/Pytorch-Handwritten-Mathematical-Expression-Recognition.git). We trained both models with crohme 2013 dataset, but after the training process, both models can't reach the theoretical performance that is posted in the paper. For this result, we have some hypothetical reasons for this bad performance. First, we are training both models on a small dataset(we have only 8835 items in the crohme 2013 dataset), it's difficult to get a well-performed model when we are lack of data to feed the model. Second, neither model is published by the writer of the papers. So there may be some small changes in the model, and there may be plenty of improvements that we can make. Since the Multi-Scale Attention with Dense Encoder model has a better theoretical performance than the WAP model, the model we're working on is based on Multi-Scale Attention with Dense Encoder model. 
The original model form GitHub has an encoder-decoder structure. The encoder part contains 2 different inputs, one has the normal resolution so the Densnet and CNN can extract the information of the structure and the symbols with bigger size, and the other has much higher resolution and it can extract the information about some details that may be neglected by in normal resolution. And the attention mechanism is applied in two different scales, by using attention, the model can focus on the informative annotations of the input images. In the decoder part, the model uses GRUs since the output is the latex form expressions, so the spatial relation plays a very important role here. GRUs can store long term memory which is very popular in some NLP tasks. And the task we are dealing with has some similar characters that are in common with NLP problems, in the math equation there is also relations between the former context and later context. The beam search is also implemented, the result is the one with the highest probability. The model has the input image size of 128*128 pixels, and the output is a latex form equation, the tokens in the expression has 118 different types. It can recognize normal plus, minus, multiply, divide, fraction, integration, root, exponent equations. But it can’t recognize matrix calculation.
After understanding the model we started to try to make the performance better. After a few tries, we got several models by applying different learning rate changing functions, the best model we got has the WER(Word Error Rate) of 40.93% and the Correct expressions rate of 1.13%, and it's still far from the theoretical performance(WER 12.9%, correct expressions rate 52.8%). So we want to analyze the cause of this poor performance. 
So intuitively we come up with 2 solutions. First is more training data, secondly is higher resolution. To justify those two solutions and find more potential factors that may affect the result, we make some statistical work to see which type of expression or token is more likely to be wrongly recognized. Besides, we also checked the wrongly recognized expressions to see if there is any feature that is in common. And the statistical analysis can also show a more accurate evaluation of the models. By comparing the statistical results of different models we can find out the improvements and the differences. As a result, each time we trained a new model with different hyperparameters, different datasets or different model structure, we always check the WER and  Correct expressions rate  as an intuitive index at first, and if it’s worth analysing, we make the statistical analysis of the testing result.
The statistical analysis is divided into two parts, one is the analysis on the expression level, it may have macro perspective of how the expressions are recognized and different recognition result of expressions with different length, and the other one is on the token level. 
Expression analysis:
We counted the length of the actual expressions, the length of the recognized expressions, and the Levenshtein distance between those corresponding expressions. And we counted these three distances in two different conditions: 1. full expression that includes all the spatial tokens, it can provide an overview of the error source 2. The symbol only, it can provide the error source of the symbol itself. By comparing these statistical 
The pics below show the distribution of expressions with different lengths.
 

Figure1:
Figure2:
From the comparison of two distributions, we can find out that the number of long expressions in the actual label is more than the number of long expressions in the recognized results. That means that the long expressions may be recognized as some shorter expressions. And the expression length distribution that is between 0 to 10 has some significant change. That could mean the images that contain very little information may be also recognized wrongly. Which means that the data preprocessing is possibly needed, so that the image that contains the few tokens can be resized into normal size, or we can and another scale in the model, since the model now contains only 2 scales, which are normal resolution and high resolution. If we add a low resolution scale in the model, it can extract the information better when it has a higher view of the images.


Figure3:

From the distribution of the Levenshtein distance, we can draw the conclusion: most of the error length stays in the intervals of 0 to 10, but there still are many severe mistakes that have the Levenshtein distance that’s bigger than 20, which means a total false.
 
 
Tokens analysis:

 
 
 
 
Analysis of the error recognitions that have severe problems:
We checked the Figures that have big Levenshtein distance, if the distance is smaller than 5, we just consider it as some acceptable error. After sampling from the error recognitions, we found some common features of the misrecognized expressions: either the formula has some complicated spatial structure, or the formula is very long, that with the resolution of 128*128 it’s even hard for a human to distinguish the symbols in the image. And some other kind of errors often occurs, one important type is similar tokens, like ‘z’ and ‘2’, uppercase tokens and lowercase tokens, etc.
 
With the hypotheses above we tried the model with higher resolution(256*256) and more data, the outcome is better than the previous model. This model has the WER of 32.29% and the correct expression rate of 16.04%.

To evaluate the outcome of the new model, we also did some statistical analysis on the testing result, in expression level and token level. 
Expression analysis:
In the figure  56?????  we can see some very obvious improvements of the performance. The problems
we found in the low resolution model are prominently relieved, for example: many long expressions are recognized as shorter expressions, the misrecognition of short expressions. By comparing both figures, the distribution of expression lengths in truth and in recognition is similar, and this is only a sketchy approximation. 






For the more accurate compare, we recalculated the Levenshtein distance of each expression and its corresponding recognized expression. The distribution of the Levenshtein distance has 2 spikes, one is the 0, which means that there is no difference between truth and recognition and the recognition is totally right, and another spike is at 4, which means that most (80%) wrongly recognized expressions lies in interval [1,12] and have the peak at 4. 









IV.discussion and conclusion







